#!/bin/bash -e
#
# S2I run script for the 'billrainford/tensorflowapp-sample' image.
# The run script executes the server that runs your application.
#
# For more information see the documentation:
#	https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md
#

set -e

echo "Running app..."

echo "PWD = "$PWD
echo "ls -l /opt/app-root/src/"
ls -l /opt/app-root/src/
#echo "MODEL_NAME = "$MODEL_NAME
#echo "MODEL_PATH = "$MODEL_PATH
#echo "model_base_path = /opt/app-root/src/"

echo "ls -l /opt/app-root/"
ls -l /opt/app-root/

#export CUDA_HOME="/usr/local/cuda"
#export CUDA_PATH="${CUDA_HOME}"
#export PATH="${CUDA_HOME}/bin${PATH:+:${PATH}}"
#export LD_LIBRARY_PATH="${CUDA_HOME}/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}";
#echo -e “$CUDA_HOME \\n $CUDA_PATH \\n $LD_LIBRARY_PATH”

#echo " -----> Run tensorflow server."

#binary gets copied to  /opt/app-root/src/tf/tensorflow_model_server
#/opt/app-root/src/tf/tensorflow_model_server --port=6006 --model_name=$MODEL_NAME --model_base_path=/opt/app-root/src/$MODEL_PATH

echo "---> Running Inference:"
exec python /opt/app-root/src/tensorflowapp-sample/tensorflowapp-inference.py --prefix mnist- --inference_path /opt/app-root/src/input/test/test.png --saved_model_name my_saved_model /opt/app-root/src/output /opt/app-root/src/output
